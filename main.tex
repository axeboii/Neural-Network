\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx} 
\usepackage{float}
\usepackage{biblatex}
\usepackage{wrapfig}
\usepackage{subfig}
\usepackage[normalem]{ulem}
\usepackage[bottom=1in,top=1in]{geometry}
\usepackage[framed,numbered,autolinebreaks,useliterate]{mcode}
\usepackage{hyperref} 
\usepackage{caption} %
\setcounter{secnumdepth}{4}
%\titleformat{\paragraph}%{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
%\titlespacing*{\paragraph}{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}
%\useunder{\uline}{\ul}{}

\pagenumbering{arabic}
\title{\huge\textbf{Implementation and Optimisation of a Neural Network}  \\[2ex]
\extaLarge How stepping schedules affect learning}
\author{Axel Boivie, aboivie@kth.se\\Victor Bellander, vicbel@kth.se}
\date{April 2021}

\begin{document}

\begin{figure}[t]
\centering
    \includegraphics[width=0.3\textwidth]{Bilder/KTH_Logotyp_RGB_2013.png}
    \label{fig:logo}
\end{figure}

\maketitle
\thispagestyle{empty}

\vfill{\noindent SA115X Degree Project in Vehicle Engineering, First Level, 15.0 Credits\\
Supervisor: Anna Persson\\
KTH, Stockholm\\
Spring 2021}

\newpage

\begin{abstract}
\noindent As this project contains an implementation and optimisation of a basic neural network, it covers most of the essentials within the field of neural networks and machine learning. The focus however, lay on studying how stepping schedules affect learning. A stepping schedule is an in beforehand determined way to decrease the learning rate over the training process. It turns out that for the networks implemented, stepping schedules does not seem to make a significant difference in performance, although very slight improvements from constant learning rates could be found in some cases. This event kept repeating itself for different depths and widths for the neural network, and the type of stepping schedule did not seem to have much of an impact. This result is somewhat surprising, as we in theory would expect to see a rise in performance when learning rate is gradually decreased over learning. The code for the implementation is found through the following link
\url{https://github.com/axeboii/Neural-Network}.
\end{abstract}
%in/on
\newpage
\section*{Preface}
This report was written as a bachelor degree project in Vehicle Engineering at KTH, Royal Institute of technology, Stockholm, Sweden. Before entering into this project we had no previous experience in the field of machine learning in general or neural networks in particular. The project has been a huge learning procedure for us where we have used knowledge from a mix of courses, mainly within mathematics, numerical methods and programming. We hope that you will find the report interesting. 

\newpage
\tableofcontents
\newpage
\section{Introduction}
In today's society, machine learning plays an increasingly important role. Classification is an important machine learning task, whether it regards speech recognition, face detection, document classification or in the case of this project, handwriting recognition. A neural network is a machine learning algorithm well suited for such a task. In this project, a neural network will be implemented, trained and tested with labelled handwritten digits (MNIST dataset), with the goal of studying how \textit{stepping schedules} can help optimise training. A stepping schedule is an in beforehand determined scheme to decrease the learning rate over a training process, which can help maximising progress in training. A challenge for neural networks today is optimising training, as accurate networks with reasonable computational demands are desirable. Hopefully, a stepping schedule could help achieving this.
\subsection{Issue}
Issues being dealt with in this report are the following:

\begin{itemize}\label{issueList}
  \item Does a stepping schedule improve the performance of the training of a neural network? 
  \item Will a stepping schedule improve the classification of data made by the neural network?
  \item If a stepping schedule proves to be beneficial to use, does it hold for all circumstances?
  \item How do stepping schedules hold up to adaptive optimisation methods, such as ADAM?
\end{itemize}

\subsection{Delimitations}

When investigating the effect of using a stepping schedule while training a neural network, a few delimitations have been done. The delimitations are presented below:

\begin{itemize}
    \item Only fully connected neural networks have been studied.
    \item Only three different sizes of neural networks have been analysed, with 2 layers maximum.
    \item Only five traditional stepping schedules and one adaptive optimisation algorithm have been tested.
    \item Only one dataset have been used.
\end{itemize}

% Endast tre olika storlekar på nätverk kommer att analyseras
% Endast ett visst antal stegscheman testas
% Endast en typ av adaptiv stegschema skall testas
% Endast test på ett dataset har använts
% Endast 

\subsection{Background}
\noindent As the field of neural networks is so comprehensive, this background section will focus on theory related to the implementation of a neural network with the objective of classifying images from the MNIST Dataset.

\subsubsection{Neural networks}
A neural network (NN) is a complex type of machine-learning algorithm. The name comes from its' resemblance of a biological neural network, such as those found in our own brains, where neurons are linked together. Similarly, a neural network consists of nodes linked together in a network-like fashion. The idea is that these nodes are organised in layers, with a input layer, an arbitrary number of hidden layers and an output layer. These layers consist of a number of nodes, and connections to the next layer. There are too many types of neural network structures for this report to handle, so this report is limited to fully-connected neural networks (FCNN:s). This means that every node at a specific layer is connected to all the nodes at the next layer. Networks in this report will rely on supervised learning, meaning all training data are labelled with its' correct answer. (Goodfellow et. al, 2016)

\noindent In a mathematical way, a general neural network can be described as a function $\mathbf{f}(\mathbf{x}; \mathbf{\theta}) : \mathbb{R}^m \xrightarrow{} \mathbb{R}^n$, where '$\mathbf{x}$' is the input to the function, '$\mathbf{f}$' the output and '$\mathbf{\theta}$' are the parameters that determines the network. In the process of resolving which hand-written digit is on a 28 by 28 pixel image, this function can be further clarified as the function $\mathbf{f}(\mathbf{x}; \mathbf{\theta}) : \mathbb{R}^{784} \xrightarrow{} \mathbb{R}^{10}$. The input is of dimension 784, as there are 784 input pixel values which ranges from 0 to 1 depending on it's brightness. The output is of dimension 10 as there are 10 digits 0-9, with each output node displaying a number 0-1 depending on how certain the network is on which digit the image depicts. Furthermore, the parameters $\theta$ are determined by the architecture of the network, described in the following section.

\paragraph{Architecture of a neural network}\\
\hfill \break
\begin{figure}[H]
    \centering
    \includegraphics[scale = 0.7]{Figurer/Neural network.jpg}
    \caption{This figure illustrates how a basic neural network is built.}
    \label{fig:brabild}
\end{figure}
\noindent The basic architecture of a fully connected neural network is showcased in the figure above. The number of hidden layers and nodes in each of them is arbitrary, although an input layer with 784 nodes and and output layer of 10 nodes are required for this particular problem. A node in this case is simply a value, usually ranging from 0 to 1.  This value is referred to as it's \textit{activation}. \\

\noindent The connection of one node to another node requires a \textit{weight}. The weight determines how much the previous node's activation affects the current node's activation, mathematically described as $a^l = w\cdot a^{l-1}$, with the connection's weight as '$w^l$', and activations given as '$a$', with superscripts '$l$' and '$l-1$' indicating which layer the nodes belong to. A common initialisation of the weights, for a network with m inputs and n outputs, is to use the Glorot initialisation, presented below. (Goodfellow et. al, 2016)\\
\begin{equation}\label{eq: glorot weight initialisation}
    w \thicksim U\left(-\sqrt{\frac{6}{m+n}},\sqrt{\frac{6}{m+n}}\right) \approx U\left(-0.087,0.087\right)
\end{equation}

\noindent Additionally, we might require a threshold, under which no activation is given at the current node. This is regulated using a \textit{bias}, which using previous notation gives  $a^l = w\cdot a^{l-1}+b$, where '$b^l$' is the current node's bias. It is not uncommon for networks to not have biases at all, although the one used in this problem has. Biases are usually initialised to zero. (Goodfellow et. al, 2016)\\

\noindent Lastly, we need to control the current node's activation to land between 0 and 1 again, so we run this value through an \textit{activation function}, which accomplished this. The formula for the activation of one node can now be completed, with indexes $j$ and $k$ added to keep track of which nodes are used at the current and previous layers. With $\sigma$ representing the activation function, the current node's activation is calculated through

\begin{equation}\label{eq:singleactivation}
    a^l_j = \sigma(a^{l-1}_k\cdot w_{j,k}^l + b^l_j).
\end{equation}
\iffalse
\begin{figure}[H]
    \centering
    \includegraphics[scale = 0.3]{Figurer/NN_slice.png}
    \caption{Caption}
    \label{fig:my_label}
\end{figure}
\fi
\noindent One common activation function is the sigmoid function, which is defined by
\begin{equation}\label{eq:sigmoid}
    \sigma_{sigmoid}(x) = \frac{1}{1+e^{-x}}. 
\end{equation}
The sigmoid function gives a number close to 0 for small, or negative x, and a value close to 1 for large x. Another common choice for an activation function is the rectified linear unit function (ReLu). This is defined as
\begin{equation}\label{eq:ReLu}
    \sigma_{ReLu}(x) = max(0,x). 
\end{equation}
The ReLu function cannot give negative values, but can give infinitely large positive values, so the output range is not 0-1 as it is for the sigmoid function. This may seem problematic, but in practice it works fine as the output is still compared with the correct value, which is within the range 0-1. In modern neural networks the default recommendation is to use ReLu (Goodfellow et. al, 2016, p.171). ReLu is more computationally efficient as it just needs to pick $max(0, x)$, compared to the exponential operations for sigmoid. Additionally, the gradient of the sigmoid function vanishes for very small or large x, which is not the case for ReLu as it is either 0 or 1. The advantage of the sigmoid function is, as previously mentioned, that activation is limited between 0 and 1, and the risk of blowing up activation is avoided. \\
%https://www.datasciencecentral.com/profiles/blogs/deep-learning-advantages-of-relu-over-sigmoid-function-in-deep

\noindent If equation (\ref{eq:singleactivation}) is used for all activations in a layer, it can easily be formed into matrix-form, with the current layer's activations as $\mathbf{a}^l$, the previous layer's activations as $\mathbf{a}^{l-1}$, all the weights of the connections between the nodes in each layers as $\mathbf{w}^l$ and all the biases at the current layer as $\mathbf{b}^l$, as
\begin{equation}\label{eq:act}
    \mathbf{a}^l = \sigma(\mathbf{w}^l\cdot\mathbf{a}^{l-1} + \mathbf{b}^l).
\end{equation}
Each node, except for the input nodes, is associated with it's own bias, and each connection with it's weight. Collectively, all the weights and biases of a network forms the parameters $\mathbf{\theta}$ referenced earlier. The dimension of $\theta$ can easily grow very large, as showcased in a few examples below.\\

\noindent If the NN used in the current task of image classification and the NN had no hidden layers, but just an input layer of 784 nodes and output layer of 10 nodes, this would give $784\cdot10 = 7840$ weights and $10$ biases, and the dimension of $\theta$ would be \textbf{7850}. If one hidden layer of 32 neurons are added between them, this would give $784\cdot32 + 32\cdot10 = 25408$ weights and $32+10 =42$ biases, giving the dimension of $\theta$ as \textbf{25450}. If a NN with 2 hidden layers of 400 neurons are used this gives $784\cdot400 + 400\cdot400 + 400\cdot10 = 477600$ weights and $400+400+10=810$ biases, giving $\theta$ a dimension of \textbf{478410}. You easily see how the number of parameters grows rapidly with larger networks. This plays a large part in the computational demands of training a NN, as can be seen in the following section which describes this. 
\paragraph{Training}\label{Training}
\hfill \break

\noindent The goal of this NN is predicting which digit is written on a given image. To accomplish this, the NN obviously needs training. The process of training a NN is the central problem in both implementation and optimisation of a NN. By 'training' the network, we mean the process of fitting all the weights and biases (gathered in $\theta$) of the network to a given set of data. As the parameters $\theta$ are initialised randomly, an untrained NN will give random results when an image is put through the system. Now, training is done by putting images through the network and comparing the results with the desired results, which is possible as each image is labelled with it's supposed digit. If the result is close to the desired result only small tweaks are made to the network, and vice versa if the result is far from the desired result. The comparison in done through a \textit{loss-function}, also called a cost-function. One such loss-function is mean square error (MSE). With the last layer's activations in $\mathbf{a}^L$ (output of the network) and the desired values in $\mathbf{y}$ the cost $C$ using MSE in this problem is calculated through
\begin{equation}\label{eq:MSE}
    C = \frac{1}{N}\sum^{N}_{i=1}(a_i^L-y_i)^2.
\end{equation}
\noindent Here $N$ is the number of nodes in the output layer, which will be 10 for this particular problem. The values in $\mathbf{y}$ will be 0 for all values except for the correct one, which will be 1. If the digit of an image is a six, this means $y$ is given as $[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]$. If the network is $95\%$ sure the image is a six, and almost sure it's not any other digit, the output $\mathbf{a}^L$ might look something like $[0.01, 0.03, 0.02, 0.02, 0.01, 0.04, 0.95, 0.03, 0.06, 0.03]$. The problem of training the network now becomes a minimisation problem of the cost $C$. If $C$ is thought of as a function $C\left(\mathbf{f}(\mathbf{x}; \mathbf{\theta}), \mathbf{y}\right)$, as $\mathbf{a}^L$ = $\mathbf{f}(\mathbf{x}; \mathbf{\theta})$, the problem of minimising $C$ with respect to parameters $\theta$ can be accomplished through \textit{gradient descent}. The idea of gradient descent is that the gradient of the function $C$ with respect to parameters $\theta$ gives the direction of the steepest increase of the function $C$. This gives the direction of the steepest decrease as the negative of the gradient. Iterating this process through all data means that one step in the training process is calculated through
\begin{equation}
    \theta_t = \theta_{t-1} - \epsilon\cdot\nabla_\theta\left( C\left(\mathbf{f}(\mathbf{x}_t; \mathbf{\theta}_{t-1}), \mathbf{y}_t\right)\right).
\end{equation}
\noindent Here, $\epsilon$ denotes the learning rate, or step size of the training process. This parameter is also the main focus of this report, and will be investigated in much further detail further on. Arrays $x_t$ and $y_t$ denotes the current image's pixel values as well as it's label. If this process is completed for all training images, we should theoretically reach some minimum value of the cost function. Computing the gradient of $C$ with respect to the 478410 parameters in $\theta$ for 60000 training images, as in the case of 2 hidden layers of 400 nodes, will not prove effortless. A lot of optimisation strategies must be deployed to make this problem computationally achievable, the most rudimentary of which is \textit{stochastic gradient descent}, described in the following section. (Goodfellow et. al, 2016)\\

\noindent The goal of training is obviously for the network to be able to perform well on previously unseen data, the testing dataset in this case. When a network becomes 'too' well-trained over the training dataset, to the detriment of testing performance is when \textit{overfitting} occurs. A way to illustrate overfitting is studying the gap between testing and training error, which becomes very large when the network is overfitted. The opposite of overfitting is \textit{underfitting}, which is simply when the network is not well-trained enough. This theory explains why a network cannot simply keep training over the same training dataset to improve real world performance. (Goodfellow et. al, 2016)


\subsubsection{Stochastic gradient descent}\label{sec: SGD}

Stochastic gradient decent, also known as 'SGD', is the most commonly used optimisation algorithm in deep learning today. Using SGD is a way of making a compromise between accuracy and speed when it comes to calculating the gradient of the cost function at a specific location. (Goodfellow et. al, 2016)\\

\noindent In theory it is possible to calculate the exact value of the gradient, and thereby direction, of every step supposed to be taken in trying to find a minimum of the cost function. This would be done simply by using all data available in a dataset when calculating the gradient in the training of the network. However, as datasets used to train neural networks are often large, it is costly to perform this type of calculation. In summary this approach leads to a very 'cautious' training algorithm, taking slow and steady steps in exactly the right direction before eventually finding a minimum of the function.(Goodfellow et. al, 2016)\\

\noindent The opposite philosophy by which to find the minimum would be to, instead of using the whole dataset to compute the the exact gradient at every step, only use a single data point to execute a very rough approximation of the gradient at this location. This approach makes for a fast but inaccurate algorithm. One could hope that as more and more steps are taken, the algorithm converges to a minimum even though the steps individually are not presentable as the correct direction of the gradient. In summary this leads to a fast, but somewhat uneven stepping pattern to the minimum compared to the first described approach.(Higham et. al, 2019)\\

\noindent SGD is a blend of the two algorithms above. With SGD every gradient is calculated using a \textit{mini-batch} of data. A mini-batch is a share of the data in the dataset used to give a good representation of the correct gradient, without having to use all the data in every step. This makes for a faster training session as less data has to be analysed. Simultaneously, the path towards the minimum is hopefully shorter and more accurate than if only one data point is used to determine the gradient. (Goodfellow et. al, 2016)\\

\noindent A mini-batch consists of a number of randomly chosen data points from the dataset. The mini-batch can be created with different philosophies. One is to build it up by first shuffling the original dataset and thereafter picking the 'm' first data points to perform the gradient approximation before taking the first step towards the minimum. For the second step, a mini-batch consisting of data points 'm+1' to '2m' is used, and so fourth. Another course of action is to randomly select each data point into each mini-batch. This is analogue to drawing 'm' number of cards from an ordered deck of cards and then putting them back into the pile after having used them. The most prone difference of the two then is the fact that first shuffling the dataset will only make use of a single data point once, whilst randomly picking from the 'deck' would make it possible for one data point to be present in several mini-batches, and some not present at all. (Goodfellow et. al, 2016)


% Beskriv filosofin för att välja bilder
% Batch size/mini-batch
% Epoch


\paragraph{Back propagation}\label{sec: backprop}\\

\hfill \break

\noindent When performing SGD, the gradient of the cost function with respect to biases and weights needs to be calculated. This is done by \textit{back propagation}, which essentially is the training procedure of the network. The main idea is to find the direction in which to take a step in parameter space in order to minimise the cost function. The goal is to get as close as possible to the optimal setting of weights and biases which would yield a cost function value of zero. This would in theory be when the network performs at it's best.\\\\
\noindent Wanting to find the gradient of the cost function, the partial derivatives of this function with respect to each $w^{l}_{jk}$ and $b^{l}_j$ in the network, has to be calculated. Here, '$l$' represents the layer in which the weight or bias is present. In the case of the bias, the '$j$' represent node number in layer $l$. In the case of the weights, $w$ connects node '$k$' in layer $l$ with node $j$ in layer '$l + 1$'. (Higham et. al, 2019)\\

\noindent Recalling from section \ref{Training}, the expression of the cost function in equation (\ref{eq:MSE}) gives us the dependence of the cost in regard to the value of the nodes at the output layer. From equation (\ref{eq:MSE}) it is seen that the cost is only dependent on the weights and biases through the value of the output layer $a^L$ (Higham et. al, 2019). Further, it is convenient to define a new variable '$z$' as the value of a node in layer $l$ contributed by the value of the corresponding nodes from the previous layer, weights and biases

\begin{equation}
    \mathbf{z}^l = \mathbf{w}^l\cdot\mathbf{a}^{l-1} + \mathbf{b}^l.
\end{equation}

%\noindent Practically $z^l$ and $b^l$ is a vector of size [j$\times$1], where j is the number of nodes in layer $l$. $W^l$ is a matrix of size [j$\times$i] and $a^{l-1}$ is a vector of size [i$\times$1]. Here i is the number of nodes in layer $l-1$.
\noindent Further the definition of $a^l$ is remembered from  equation (\ref{eq:act}), giving

\begin{equation}
    \mathbf{a}^l = \sigma(\mathbf{z}^l)
\end{equation}

\noindent with $\sigma$ as the activation function. At last we define node \textit{error} $\delta^l_j$ as

\begin{equation}
    \delta^l_j = \frac{\partial C}{\partial z^l_j}
\end{equation}

\noindent to describe how sensitive the cost function is to a change in the value of z of node $j$ at layer $l$. Finally, this yields the four equations of back propagation
\begin{equation}\label{eq:bp1}
\begin{aligned}
    &\delta^L = \Delta_a C \odot \sigma'(z^L)\\[1ex]
    &\delta^l = ((w^{l+1})^T)\delta^{l+1}) \odot \sigma'(z^l)\qquad\qquad &\textrm{for}\quad2\leq l \leq L-1\\[1ex]
    &\frac{\partial C}{\partial b_j^l} = \delta^l \qquad &\textrm{for}\quad2\leq l \leq L\\[1ex]
    &\frac{\partial C}{\partial w_{j,k}^l} = a_k^{l-1}\delta^l &\textrm{for}\quad2\leq l \leq L,
\end{aligned}
\end{equation}

\noindent all derived from the chain rule. For the full derivation of the equations (\ref{eq:bp1}), see reference (Higham et. al, 2019).\\

\noindent Lastly, the notation $\odot$ needs to be described. The sign denotes a \textit{Hadamard product} which is an element wise multiplication of two vectors of the same size. This means that in a Hadamard multiplication of vectors $x$ and $y$ ($x\odot y$), the component $i$ of the Hadamard product is calculated though $(x \odot y)_i = x_i\cdot y_i$. (Higham et.al, 2019)

\paragraph{Noise ball}\label{sec: noiseball}

\hfill \break

\noindent The theory behind the idea that large learning rates will not get close to a minimum with SGD can be explained by introducing the term \textit{noise ball}. As the number of iterations in training goes to infinity, SGD converges to a noise ball. This means that the step size is too large to enable SGD to get any closer to a minimum, and 'bounces' around the minimum. The size of the noise ball is proportional to the learning rate $\epsilon$. Mini-batching can minimise the noise ball additionally. For accurate solutions, a small noise ball is ideal. There is however a trade-off, as a low learning rate increases convergence time. For the proof of noise ball, see reference (Cornell CS, 2017).

\paragraph{Learning rate and stepping schedules}\label{sec: Schedules}

\hfill \break

\noindent One of the most important parameters when setting up the training of a neural network is the learning rate $\epsilon$. The learning rate is the parameter deciding the step size by which to go in the opposite direction of the gradient. The value of the learning rate is important in the way that a well chosen learning rate will be beneficial for the learning procedure. An $\epsilon$ too large will make the learning procedure very oscillative, converging fast to a large noise ball. On the other hand, a small $\epsilon$ makes the learning slow, though delivering a relatively smooth path towards the minimum. \\

\noindent An issue when dealing with SGD in tandem with a constant learning rate during a whole training session is the fact that SGD introduces noise in the gradient calculations. This is alright as long as the training algorithm is not close to a local minimum in the cost function. When closing in on a minimum though, the noise of the gradient calculation will be proportionally larger to the actual gradient leading to taking random steps around the minimum. To counteract this phenomenon a none constant learning rate could be helpful. The denotion of $\epsilon$ could now be changed to $\epsilon_k$, where $k$ ranges from $1$ to $n$ during $n$ iterations of the training process. How $\epsilon_k$ changes during training is defined by a \textit{stepping schedule} providing a good set of values for the learning rate. More on this later. (Goodfellow et. al, 2016)\\

\noindent In theory, not using SGD but the whole dataset, to calculate the gradient in each step would make a stepping schedule superfluous. This is due to the fact that this method, though slow, will produce the exact value of the gradient at each location of parameter space. This means that close to a local minimum, the gradient will be small and at that minimum, the value of the gradient would be zero.(Goodfellow et. al, 2016) \\

\noindent There are a number of different stepping schedules providing more stability and reducing the impact of noise close to minimums of the cost function. To begin with there are a number of set schedules depending on mathematical functions. Examples of these types of schedules are \textit{exponential decay} (equation (\ref{ExponentialDecay})), \textit{inverse time decay} (equation (\ref{InversetimeDecay})), \textit{piece wise constant decay} (equation (\ref{PieceWiseConstant})) and \textit{polynomial decay} (equation (\ref{PolyomialDecay})) (Tensorflow, 2021). Below, the formulas of calculating the present learning rate can be seen.
\begin{equation}\label{ExponentialDecay}
    \epsilon_k = \epsilon_0 \cdot \gamma^k
\end{equation}

\begin{equation}\label{InversetimeDecay}
    \epsilon_k = \frac{\epsilon_0}{1 + \gamma^k}
\end{equation}

\begin{equation}\label{PieceWiseConstant}
    \epsilon_k = (\epsilon_0 - \epsilon_n) \cdot
          (1 - \frac{k}{n})
          + \epsilon_n
\end{equation}

\begin{equation}\label{PolyomialDecay}
    \epsilon_k = (\epsilon_0 - \epsilon_n) \cdot
          (1 - \frac{k}{n}) ^ p
          + \epsilon_n
\end{equation}

\noindent Above, '$\epsilon_0$' is the initial learning rate, '$\epsilon_n$' is the end learning rate, '$\gamma$' is the \textit{decay rate}, '$k$' is the present step in the training session, '$n$' is the total number of steps and '$p$' is a selectable coefficient.\\

\noindent As a part of implementing a stepping schedule, it is central to know when theses learning rates should be updated. This can be done in several ways. One way could be to update the learning rate after each mini-batch and therefore after each taken step. Another philosophy could be to update the learning rate after each \textit{epoch}, which is another term needing an explanation. An epoch is defined as when an entire dataset has been passed through the neural network once in the training session (Sharma, 2017). Usually the training session consists of several epochs, meaning the whole dataset will have been used to train the neural network several times in trying to get a sufficiently accurate network.\\

\noindent It can be shown that inverse time decay, given by equation (\ref{InversetimeDecay}), is an optimal step size scheme for convex problems (Cornell CS, 2017). However, as neural network training is not ordinarily a convex optimisation problem, this stepping schedule might not be optimal for this particular problem.
% Learning rate och något om stegscheman. Varför stegscheman?

% Epoch

\paragraph{Adaptive learning rate and ADAM}\label{adaptiveADAM}

\hfill \break

\noindent The stepping schedules presented in the sections above does not take into account what the surrounding of the cost function look like or what step sizes and gradients have been used before. From an optimisation stand point it could be beneficial to choose $\epsilon_k$ based on these criteria and not only follow a fix mathematical formula. This can be done using an algorithm  with an adaptive learning rate.\\

\noindent One of the most respected adaptive learning rate algorithm today which have shown to be effective in a lot of different deep learning implementations is the \textit{ADAM algorithm} (DeepLearningAI, 2017). ADAM in itself is a continuation and development of two other optimisation algorithms called \textit{momentum} and \textit{RMSprop} (Bushaev, 2018).\\

\noindent The basic idea behind momentum is to take into account not only the value of the gradient at the present location, but also let it be influenced by an exponentially decaying moving average of previous values of the gradient. In theory this means speeding up learning in especially steep or ravine-shaped regions in parameter space. A momentum implementation could also conquer a small hill to later finding its way to a new 'better' minimum, contrary to a non adaptive stepping schedule getting stuck. (Goodfellow et.al, 2016)

\begin{figure}[H]
    \centering
    \includegraphics[scale = 0.3]{Figurer/Momentum.png}
    \caption{Shows difference in behaviour between momentum and non-momentum implementation.}
    \label{Momentum}
\end{figure}

\noindent The RMSprop algorithm calculates an appropriate step size in parameter space by scaling all the model parameters, weights and biases, 'inversely proportional to the squareroot of the sum of all the historical squared values of the gradient [and] changing the gradient accumulation into an exponentially weighted moving average.'(Goodfellow et.al, 2016, p. 303). By doing this, RMSprop changes learning rate as the training proceeds but only as much as the nearest history of surroundings suggest. This is favourable as the shape of parameter space can change during the path towards a minimum, requiring an ability to local adaptation in the learning rate.(Goodfellow et.al, 2016)\\\\
ADAM is as mentioned a combination of the two algorithms explained. The first step of the algorithm is to calculate the influence of momentum  and RMSprop on the step size. 

\begin{equation}\label{RMSMom}
\begin{aligned}
    V_{{dw}_t} &= \beta_1 V_{dw_{t-1}} + (1 - \beta_1) dw_t\\
    S_{{dw}_t} &= \beta_2 S_{dw_{t-1}} + (1 - \beta_2) dw_t^2\\
    V_{{db}_t} &= \beta_1 V_{db_{t-1}} + (1 - \beta_1) db_t\\
    S_{{db}_t} &= \beta_2 S_{db_{t-1}} + (1 - \beta_2) db_t^2
\end{aligned}
\end{equation}

\noindent Equation (\ref{RMSMom}) shows formulas to calculate the following: '$V_{{dw}_t}$' and '$V_{{db}_t}$' which are the momentum exponentially weighted average at step $t$ for weights and biases. '$S_{{dw}_t}$' and '$S_{{db}_t}$' which are the RMSprop exponentially weighted average at step $t$ for weights and biases. In equation (\ref{RMSMom}) $dw$ and $db$ are the gradient of the cost function with respect to weights and biases calculated with back propagation at step $t$. '$\beta_1$' and '$\beta_2$' are two hyperparameters set to $\beta_1 = 0.9$ and $\beta_2 = 0.999$. At the start of the training the values of $V_{{dw}_t}$, $S_{{dw}_t}$, $V_{{db}_t}$ and $S_{{db}_t}$ are set to be 0. This yields that at iteration $t = 1$, the values of $V_{{dw}_{t-1}}$, $S_{{dw}_{t-1}}$, $V_{{db}_{t-1}}$ and $S_{{db}_{t-1}}$ are all zero. (DeepLeraningAI, 2017)\\\\
\noindent It is necessary to correct the calculated parameters in equation (\ref{RMSMom}) from a certain degree of bias. How this is done is shown below in equation (\ref{RMSmomCorr}). (DeepLeraningAI, 2017)

\begin{equation}\label{RMSmomCorr}
\begin{aligned}
\hat{V}_{{dw}_t} &= \frac{V_{{dw}_t}}{1 - \beta_1^t}\\
\hat{S}_{{dw}_t} &= \frac{S_{{dw}_t}}{1 - \beta^t_2}\\
\hat{V}_{{db}_t} &= \frac{V_{{db}_t}}{1 - \beta_1^t}\\
\hat{S}_{{db}_t} &= \frac{S_{{db}_t}}{1 - \beta^t_2}
\end{aligned}
\end{equation}

\noindent Finally, the update of the weights and biases are as follows in equation (\ref{UpdateADAM}).

\begin{equation}\label{UpdateADAM}
\begin{aligned}
w_t &= w_{t-1} - \alpha \frac{\hat{V}_{{dw}_t}}{\sqrt{\hat{S}_{{dw}_t}} + \lambda}\\
b_t &= b_{t-1} - \alpha \frac{\hat{V}_{{db}_t}}{\sqrt{\hat{S}_{{db}_t}} + \lambda}
\end{aligned}
\end{equation}

\noindent Here, '$\alpha$' is a parameter that has to be tuned for the specific neural network and problem. $\lambda$ is a parameter to avoid division by zero, set to $10^{-8}$.

\subsubsection{TensorFlow, Keras and MNIST}\label{sec: tensorflow}
TensorFlow is an end-to-end open source platform for machine learning by Google. Specifically, an interface within TensorFlow, Keras is used in this project. Keras is a deep learning API for Python, with a library of functions for building, training and testing NN:s. Within Keras there are multiple datasets built-in, which can be used for training the network. For this project, mainly the MNIST dataset was used, MNIST for 'Modified National Institute of Standards and Technology'. The MNIST dataset consists of 70000 images of hand-written digits, 28 by 28 pixels in size, all labelled with what images these are supposed to be. Of the 70000 images, 60000 are reserved for training the network and 10000 for validation and testing. Examples from the dataset are shown below. (TensorFlow,  2021)

%There is also a fashion MNIST dataset, which contains the same amount of equally large images of pieces of clothing, also labelled with what they are supposed to be. This way, the same network can be used for both datasets without any tweaking.
\begin{figure}[H]
    \centering
    \includegraphics[scale = 0.4]{Figurer/MNIST.png}
    \caption{MNIST}
    \label{Momentum}
\end{figure}

\section{Methodology}
Here follows an explanation of the methodology used in this project.
\subsection{Literature study}
Since no previous knowledge of the subject neural networks was possessed, the project began with an extensive literature study. The field of nerual networks contain many concepts, as can be seen within the background section. A good piece of literature covering all of the basics is \textit{Deep learning} by Goodfellow et. al. This book became the main reference for the first part of our literature study, offering insight into all basic and higher level aspects of a neural network. Following the first part of the study, which focused on theoretical aspects of neural networks and learning, a deeper understanding of practical forms was required. For this part, the report \textit{Deep Learning: An Introduction for Applied Mathematicians} by Higham et. al, became a large part of the study. As this report repeats many of the conceps presented by Goodfellow et. al, but also implements them as examples in MATLAB, the gap between theory and practice was reduced. Both these sources were deemed very trustworthy.\\

\noindent Furthermore... we have used non 
\subsection{Implementation}
The goal of the implementation is to build, train and evaluate a neural network in Python. Bearing this in mind, the implementation began with a simple implementation of a NN in TensorFlow. This enabled us to adjust settings such as layers, nodes, activations, stepping schedules and such in order to obtain an understanding of how each setting affects the performance of a neural network, without actually understanding the code. This played a vital role in building an understanding of a NN, and preparing for writing a NN from scratch in Python.\\\\
The process of implementing the NN from the ground up was to begin with very straightforward. A NN needs layers, with weights and biases connecting each node in one layer to each node in the next layer. They need an activation function as well as a cost function. The NN needs parameters such as layer sizes and numbers, initialisations of weights and biases. All the weights of a layer are implemented as a matrix where rows and columns corresponds to the previous and the current layers respectively. The biases of the layers are implemented as an array with one value per node in the current layer. The weight-matrices and bias-arrays for all layers are kept in basic python-lists. Now, the activation of one layer can be calculated using equation (\ref{eq:act}). With all this in place, the architecture of the NN is basically finished. The challenge becomes training the network. \\\\
The process of training the network relies heavily on the mathematical conclusions presented in the background section, namely the equations of back propagation (\ref{eq:bp1}). The implementation of back-propagation was assisted by the examples in MATLAB presented by Higham et. al, and finally yielded a function which calculated the gradient of the cost function with respect to all the weights and biases for a mini-batch. The algorithm then takes a step in the opposite direction of the gradient for all parameters, in accordance with the process of stochastic gradient descent. SGD is thoroughly presented in the background section \ref{sec: SGD}. \\\\
Next, the training process was divided into sections, with each running a certain number of mini-batches. Using SGD means that not all the 60000 images must be treated, since images are selected randomly from the dataset. This sped up the training process significantly. The default choice was to evaluate 10\% of the data (6000 images) in each section in mini-batches of 16, giving 375 mini-batches per section. Typically, 10 sections were ran, giving the total number of images treated as 60000, and the total number of batches as 3750. Further, implementing stepping schedules means that after each section, the learning rate is decreased according to the given schedule. \\\\
\noindent To compare schedules, two parameters were studied; training loss and testing accuracy. These were evaluated after each batch, with a batch size of 16. Training loss was calculated using MSE in accordance to equation (\ref{eq:MSE}) for all images in the batch. Testing accuracy was calculated using 50 random images from the testing dataset. The fraction of these correctly predicted gave the training accuracy. Using all the 60000 training images in batches of 16 in training, this gives $60000/16=3750$ batches and as many data-points for training loss and testing accuracy.

\subsection{Optimisation and visualisation}
%The optimal learning rate is not constant during different parts of the training process. In the beginning a high learning rate is ideal, as the main goal is to just find an approximate location minimum-point. As this minimum-point is approached, smaller steps are ideal in order to locate the minima with greater precision. 
%It determines how large steps are taken in gradient descent, which has the goal of finding a minima of the loss-function. Too small steps might lead to not even reaching the minima, and too large steps might mean an eventual minima is jumped over.
%The learning rate of a network is a vital parameter, as learnt in section \ref{sec: Schedules}. The optimisation consists of implementing stepping schedules, giving higher learning rate in the beginning where the minima is far away, and lower learning rate as the minima is approached. 

When investigating the stepping schedules' effect on the loss function and accuracy, a trial and error philosophy was used.  The parameters, such as learning rate and decay rate, was tuned until satisfactory properties of the network was obtained. The optimal parameter for every stepping schedule was noted and used in the evaluation of the schedules. Also, worse than optimal parameters where chosen and noted for each stepping schedule to make it possible to present the impact of the parameters in plots.\\\\
\noindent When collecting data for visualisation, ten separate learning sessions were performed to make the results more trustworthy and less dependent on randomness. The data points collected were processed so that the mean of the ten training sessions was gained. To make the results more readable the number of data points was reduced from the number of steps taken, 3750, to 50. This was made by averaging every 75 steps to generate one single data point from that information. This reduced the noisy nature of the original data. The results were presented in plots. This approach to visualisation was used independently of the test performed and shown.

\subsection{Parameter settings}

The parameters used if nothing else is noted is seen below.

\begin{table}[H]
\centering
\caption{The parameters used with the different schedules.}
\begin{tabular}{||c c c c||}
 \hline
 \textbf{Stepping schedule} & \textbf{Inital learning rate $\epsilon_0$} & \textbf{Decay rate $\gamma$} & \textbf{End learning rate} \\ [0.5ex]
 \hline\hline
 No schedule & 0.3 & - & - \\ 
 \hline
 Piece wise constant decay & 0.3 & linear & 0.055 \\
 \hline
 Exponential decay & 0.5 & 0.75 & - \\
 \hline
 Inverse time decay & 0.5 & 0.5 & - \\
 \hline
 Polynomial decay & 0.3 & 2.5 & 0.01 \\
 \hline
\end{tabular}
\label{ParameterTable}
\end{table}

\noindent When ADAM is used the parameter $\alpha = 0.003$ if nothing else is noted.

%\noindent As the background section \ref{} shows, the optimal stepping schedule for a convex problem follows equation \ref{}, often referred to as \textit{inverse time decay}. As this schedule has two parameters, both were varied to study how they affect learning.\\\\

%\noindent The stepping schedules presented in section \ref{sec: Schedules} are to be compared. Firstly, the schedules' parameters were optimised somewhat accurately, giving satisfactory settings for each one. Secondly, the network was trained from scratch using each of the four stepping schedules, with a step in learning rate applied after 375 batches, or 10\% of the data. The learning rate's value given by the respective schedules for each batch is plotted below. 

%If the two parameters in the schedule are varied, the learning rate in the training process will follow according to the plots below.

% Glöm ej litteraturstudie (Skulle aldrig)
% Hur är nätverket implementerat?
% Hur har vi gått till väga för att testa?
% Test av stegscheman på kläder/siffror med två olika storlek på närverket.
% Test av ADAM


\section{Result}
\subsection{Learning rate}\label{sec: lr res}

\noindent The two figures below showcases how learning rate affects the training process. Figure \ref{fig:loss_lrs} shows the loss of each batch, and Figure \ref{fig:acc_lrs} shows the accuracy when testing the network with test-images after each batch. No stepping schedule is used.

\begin{figure}[H]
  \centering
  \subfloat[Training loss.]{\includegraphics[width=0.5\textwidth]{Figurer/learningRates_trainLoss_noScheme.png}\label{fig:loss_lrs}}
  \hfill
  \subfloat[Testing accuracy.]{\includegraphics[width=0.5\textwidth]{Figurer/learningRates_testAcc_noScheme.png}\label{fig:acc_lrs}}
  \caption{These figures show how loss and accuracy are affected by the learning rate $\epsilon$ in the case of no stepping schedule.}
\end{figure}


\subsection{Stepping schedules}
Firstly, the inverse time decay schedule is studied. Varying the parameters $\epsilon_0$ and $\gamma$ the following learning rates are used.
%\noindent Figure \ref{StepSchedNoiseB} shows the size of the learning rate during the training session when the inverse time decay stepping schedule is used. Learning rate and decay rate is tuned independently.
\begin{figure}[H]
  \centering
  \subfloat[Learning rate with varying initial learning rate $\epsilon_0$.]{\includegraphics[width=0.5\textwidth]{Figurer/itd_learningrate.png}\label{fig:itd_learningrate}}
  \hfill
  \subfloat[Learning rate with varying decay rate $\gamma$.]{\includegraphics[width=0.5\textwidth]{Figurer/itd_decayrate.png}\label{fig:itd_decayrate}}
  \caption{These figures show how the learning rate is affected parameters $\epsilon_0$ and $\gamma$.}
  \label{StepSchedNoiseB}
\end{figure}

\noindent Now, these learning rates are implemented and gives Figure \ref{itdVary} and \ref{itdgamma}, which shows loss and accuracy corresponding to a change in initial learning rate with either a constant decay rate of $\gamma = 0.5$, or constant initial learning rate $\epsilon_0 = 0.5$.

\begin{figure}[H]
  \centering
  \subfloat[Training loss.]{\includegraphics[width=0.5\textwidth]{Figurer/itd_trainLoss_learningRate.png}\label{fig:itdtrainlr}}
  \hfill
  \subfloat[Testing accuracy.]{\includegraphics[width=0.5\textwidth]{Figurer/itd_testAcc_learningRate.png}\label{fig:itdtestlr}}
  \caption{These figures show how loss and accuracy are affected by initial learning rate $\epsilon_0$.}
  \label{itdVary}
\end{figure}
\begin{figure}[H]
  \centering
  \subfloat[Training loss.]{\includegraphics[width=0.5\textwidth]{Figurer/itd_trainLoss_decayrate.png}\label{fig:itdtraindr}}
  \hfill
  \subfloat[Testing accuracy.]{\includegraphics[width=0.5\textwidth]{Figurer/itd_testAcc_decayRate.png}\label{fig:itdtestdr}}
  \caption{These figures show how loss and accuracy are affected by decay rates $\gamma$.}
\label{itdgamma}
\end{figure}

\noindent The other stepping schedules are now studied. Figure \ref{fig:step schedule} shows how the learning rate varies during a training session for settings found in Table \ref{ParameterTable}, whilst Figure \ref{32stepcompare} shows how loss and accuracy are affected.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.3]{Figurer/learningRates.png}
    \caption{This plot shows how the learning rate varied in the different schedules.}
    \label{fig:step schedule}
\end{figure}
\begin{figure}[H]
  \centering
  \subfloat[Training loss.]{\includegraphics[width=0.5\textwidth]{Figurer/allSchemes_trainLoss.png}\label{fig:all_loss}}
  \hfill
  \subfloat[Testing accuracy.]{\includegraphics[width=0.5\textwidth]{Figurer/allSchemes_testAcc.png}\label{fig:all_acc}}
  \caption{These figures show how loss and accuracy are affected by different stepping schedules.}
  \label{32stepcompare}
\end{figure}
% Plottar, precision, stegscheman, antal lager 


\subsection{Adaptive learning rate}\label{sec: adaptive res}

\noindent ADAM is now compared to the regular stepping schedules. Figure \ref{32stepADAMcompare} shows the performance.

\begin{figure}[H]
  \centering
  \subfloat[Training loss.]{\includegraphics[width=0.5\textwidth]{Figurer/ADAM_trainLoss.png}\label{fig:adam_loss}}
  \hfill
  \subfloat[Testing accuracy.]{\includegraphics[width=0.5\textwidth]{Figurer/ADAM_testAcc.png}\label{fig:adam_acc}}
  \caption{These figures show how loss and accuracy are affected by ADAM with respect to the other stepping schedules.}
  \label{32stepADAMcompare}
\end{figure}

\subsection{Network size}\label{sec: size res}
It is interesting to study how the results might be impacted by network structures and sizes. Figure \ref{no3216400} shows loss and accuracy when using a constant learning rate $\epsilon = 0.3$ for three different sizes of the hidden layers. 
\begin{figure}[H]
  \centering
  \subfloat[Training loss.]{\includegraphics[width=0.5\textwidth]{Figurer/sizes_trainLoss_noScheme.png}\label{fig:1616_loss}}
  \hfill
  \subfloat[Testing accuracy.]{\includegraphics[width=0.5\textwidth]{Figurer/sizes_testAcc_noScheme.png}\label{fig:1616_acc}}
  \caption{These figures show loss and accuracy with no stepping scheme and hidden layers of different sizes with $\epsilon = 0.3$.}
  \label{no3216400}
\end{figure}
\noindent As the slope of the yellow 400-400 curve is so steeply declining throughout training, a test with longer training was implemented. It ran for 187500 batches, 50 times the current amount. A result of testing accuracy over 98\% was achieved, although severe overfitting was observed.\\

\noindent Implementing the inverse time decay schedule for the different sizes gives Figure \ref{itd3216400}, which shows loss and accuracy with the three network sizes. Parameters from table \ref{ParameterTable} are used.
\begin{figure}[H]
  \centering
  \subfloat[Training loss]{\includegraphics[width=0.5\textwidth]{Figurer/sizes_trainLoss_itd.png}\label{fig:1616_loss}}
  \hfill
  \subfloat[Testing accuracy.]{\includegraphics[width=0.5\textwidth]{Figurer/sizes_testAcc_itd.png}\label{fig:1616_acc}}
  \caption{These figures show loss and accuracy with inverse time decay using $\epsilon = 0.5$ $\gamma = 0.5$ and hidden layers of different size.}
  \label{itd3216400}
\end{figure}

\noindent In order to study how stepping schedules are affected by network sizes, no scheme, inverse time decay and ADAM are plotted together for the different network sizes. This gives Figures \ref{32compare3}, \ref{16compare3} and \ref{400compare3}. Parameters from table \ref{ParameterTable} have been used. 
\begin{figure}[H]
  \centering
  \subfloat[Training loss]{\includegraphics[width=0.5\textwidth]{Figurer/32_trainLoss.png}\label{fig:32_loss}}
  \hfill
  \subfloat[Testing accuracy]{\includegraphics[width=0.5\textwidth]{Figurer/32_testAcc.png}\label{fig:32_acc}}
  \caption{Loss and accuracy for a hidden layer of size 32 with respect to different stepping schedules.}
  \label{32compare3}
\end{figure}
\begin{figure}[H]
  \centering
  \subfloat[Training loss]{\includegraphics[width=0.5\textwidth]{Figurer/1616_trainLoss.png}\label{fig:1616_loss}}
  \hfill
  \subfloat[Testing accuracy]{\includegraphics[width=0.5\textwidth]{Figurer/1616_testAcc.png}\label{fig:1616_acc}}
  \caption{Loss and accuracy for hidden layers of sizes 16-16 with respect to different stepping schedules.}
  \label{16compare3}
\end{figure}
\begin{figure}[H]
  \centering
  \subfloat[Training loss]{\includegraphics[width=0.5\textwidth]{Figurer/400400_trainLoss.png}\label{fig:400_loss}}
  \hfill
  \subfloat[Testing accuracy]{\includegraphics[width=0.5\textwidth]{Figurer/400400_testAcc.png}\label{fig:400400_acc}}
  \caption{Loss and accuracy for hidden layers of sizes 400-400 with respect to different stepping schedules.}
  \label{400compare3}
\end{figure}

\subsection{Code}\label{sec: code}
The written code containing the full implementation is found in the following github repository. All generated data (loss and accuracy values) are also found here as .dat files, named with the particular parameter studied and setting used.\\
\url{https://github.com/axeboii/Neural-Network}

\section{Analysis}

\subsection{How learning rate affects learning}
Studying the results from section \ref{sec: lr res}, it is clear to see how the training of a network is dependant on the learning rate. For constant learning rate throughout training, the setting $\epsilon=0,3$ proved to be the best. A lower setting, $\epsilon = 0,05$, gives a slower learning and would probably reach the level of the higher setting after some more training. A higher setting such as $\epsilon = 0,75$ proved to not really give much learning at all, as the SGD converged to a large noise ball. These results are in line with what we would expect from theory, and a valuable lesson has been learnt as to which learning rate works best for this problem. 

\subsection{How stepping schedules affects learning}
Optimising the settings for the inverse time decay schedule, it is clear to see how initial learning rate $\epsilon_0$ and decay rate $\gamma$ affects the learning process. Figures \ref{itdVary} show that a high initial learning rate, $\epsilon_0=0,75$, gives slow learning as it converges to large noise balls in the beginning when learning rate is high. Clear jumps can be seen in the plot for this setting when the learning rate is lowered according to schedule. A lower initial learning rate, $\epsilon_0=0,05$, gives fast learning in the beginning, but soon becomes slow and never reaches high accuracy.  Figure \ref{itdgamma} show that decay rate $\gamma$ does not seem to have as large of an impact on learning as $\epsilon_0$. This is surprising, as the variation in learning rate is quite drastic, which can be seen in Figure \ref{fig:itd_decayrate}. This may be explained by the fact that the learning rate is the same for the first 375 batches, before the first step in the schedule is taken. This means that the minima has been approached sufficiently for the higher decay rates to still yield effective learning.\\

\noindent Regarding the three other stepping schedules, it is from figures \ref{fig:all_loss} and \ref{fig:all_acc} not obvious to see how they affect learning. The difference between them is very small, as can be seen from Figure \ref{fig:step schedule}. Unsurprisingly, this leads to very similar loss function values and testing accuracy between the four schedules. However, since they all are very different to a constant learning rate, it is not far fetched to expect the training process to also differ significantly. It turns out that all the schedules performs slightly better than no stepping scheme in the end and slightly worse in the beginning of training. For the default network, polynomial decay gave the best result in the end, although the margins are very small to other schedules. The optimal stepping schedule for convex problems, inverse time decay, was not optimal for this particular problem. This is not unexpected, as the training of a neural network is not a convex problem.

\subsection{How adaptive schedules affects learning}

As described in section \ref{adaptiveADAM}, the adaptive stepping schedule ADAM has a few unique properties that hopefully would improve the learning of a neural network. As can be seen in figure \ref{32stepADAMcompare} the ADAM algorithm seems to accomplish similar testing accuracies as the more traditional stepping schedules. However ADAM seems to be beneficial to the speed of the improvement of the network parameters, shown by the steeper slope in the beginning of the graph in figure \ref{fig:adam_acc}. The speed of the way the accuracy improves with ADAM may be due to a very well adapted step size to the surrounding parameter space. The ADAM algorithm is probably able to adapt better than a simpler stepping schedule in this case. In theory ADAM should have the opportunity to perform better also further into the training session. Why this is not true for this case, as can be seen in figure \ref{fig:adam_acc}, is still unclear.\\\\
\noindent Analysing figure \ref{32compare3}, \ref{16compare3} and \ref{400compare3} the same basic trends as above can be seen. The ADAM algorithm seems to have an advantage in the early stages of the training with the other stepping schedules managing to equal or better ADAM's result in the end.   

\subsection{How network size affects learning}
Looking at the results presented in section \ref{sec: size res}, some interesting points can be made. At the larger network hidden layer size, 400-400 nodes, no stepping scheme performed significantly better than with the default hidden layer of 32 nodes. At the same time, the smaller network of 16-16 nodes performed worse. Seemingly, more parameters gives a better performing network. On the contrary, the behaviour is completely different when training with inverse time decay, which is the optimal stepping schedule for convex problems. Here, the larger network of 400-400 performs worse in the beginning, and basically on par in the end of training. The same is true for the smaller network of 16-16, although it is faster in the beginning of training. The network with one layer of 32 performs best. We do also see some overfitting tendencies for the 400-400 and 16-16 networks, as their training loss is lower than the 32-network whilst having a lower testing accuracy. \\\\
\noindent Comparing optimisation schedules for the three network sizes it can be seen that inverse time decay is the slowest for all sizes, but recovers and performs on par with ADAM at least for smaller networks. For the smaller networks, no stepping scheme performs averagely in the beginning and worst in the end. For the larger network however, no schedule clearly performs the best, although ADAM is faster in the beginning. 

\subsection{Sources of error}
% Presenterat fel data?
% Fel i kod?
As the implementation of a neural network in python is quite extensive, it is of course possible that small errors can occur in code. However, as testing accuracy over 98\% was achieved it is reasonable to assume that no large errors are present in the network's implementation. \\

\noindent Another source of error concerns the drawn conclusion. It is possible that not enough different networks and stepping schedules were tested, which leads to the conclusion of whether or not stepping schedules helps optimise training possibly not being applicable for all networks. All networks implemented and tested in this project are relatively shallow, and consists of quite few nodes. It is possible that stepping schedules would make a more significant difference if deeper and wider networks were to be tested. 

\section{Conclusion}

Summing up the results it is clear that a stepping schedule does not improve learning independently of other parameters. Generally speaking, the ADAM algorithm seems too speed up initial learning but as this speed trails off, it is unclear if that is significant in an implementation situation. Looking at the other stepping schedules, they seem to perform better than the constant scheme in certain situations, but worse in others. 
%For example comparing figure \ref{32stepADAMcompare} with figure \ref{400compare3} we can seen that a stepping schedule is beneficial when the hidden layer is of size 32, but not when it is of size 400-400.\\\\
\noindent It is important to know the delimitations of this report as it is only concluded that for the tested circumstances, that a stepping schedule does not tend to improve learning significantly. Further testing is required to know whether stepping schedules are beneficial in the general case or not. If taking over from here it would be interesting to investigate the stepping schedules' impact when using other data sets together with different depths and widths of hidden layers. Also other parameters such as batch size and how often the learning rates are updated could be interesting parameters to tweak.\\\\
\noindent Lastly we could bring up some of the ethical aspects of the issues brought up in this report, or in general, to machine learning as a whole. Of course the fast development in machine learning makes for a few ethical questions needing an answer. As seen in China, governments can use machine learning and AI to control and monitor its citizens in a way that they deem suitable. This is a violation of the human right of freedom. On the other hand AI and machine learning could be very helpful in our everyday life when used in non surveillance and non war situations.


% utveckling/behov av ytterligare kunskap. vad kan göras härnäst. Varför är ADAM inte bättre?
% Skriv något PK om samhället och etik
% hjälper stegscheman?
% Nej!
% Snabbhet i inlärning
\newpage
\section*{Acknowlegdements}
We direct a great thank you to Anna Persson, our supervisor, who helped us with finding relevant literature and discussing ideas and results throughout this project. We would also like to thank Anton Sederlin who helped us setting up a Github repository for the code in this project.


% Ska vi tacka några som korrekturläste?
\newpage
\section{References}

\noindent Bushaev, Vitaly. 2018. Adam — latest trends in deep learning optimization.\\
\url{https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c}\\(Retrieved 24.03.21)\\\\

\noindent Cornell CS. 2017
https://www.cs.cornell.edu/courses/cs6787/2017fa/Lecture2.pdf\\(Retrieved 12.04.21)\\\\

\noindent DeepLearningAI. 2017. Adam Optimization Algorithm (C2W2L08). [video].\\ \url{https://www.youtube.com/watch?v=JXQT_vxqwIs}\\ (Retrieved 24.03.21)\\\\

\noindent Goodfellow, Ian; Bengio, Yoshua and Courville, Aron. 2016. Deep Learning. MIT Press.\\
\url{http://www.deeplearningbook.org} (Retrieved 19.03.21)\\\\


\noindent Higham, Catherine and Higham, Desmond. 2019. Deep Learning: An Introduction
for Applied Mathematicians. Published by SIAM.\\
\url{https://epubs.siam.org/doi/pdf/10.1137/18M1165748} (Retrieved 23.03.21)\\\\

\noindent Sharma, Sagar. 2017. Epoch vs Batch Size vs Iterations.\\ \url{https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9}\\(Retrieved 24.03.21)\\\\

\noindent TensorFlow. 2021.\\ \url{https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules}\\(Retrieved 23.03.21)\\\\

\newpage
\section{Appendix}
\subsection{Word list}
\textit{Activation function} - The function $\sigma$ used to control the activation of a node, see equation (\ref{eq:act}).\\ [0.75ex]
\textit{ADAM} - An optimisation algorithm which uses an adaptive learning rate, see section \ref{adaptiveADAM}.\\[0.75ex]
\textit{Back propagation} - The method used for updating weights and biases of a network in training, see section \ref{sec: backprop}.\\[0.75ex]
\textit{Bias} - A node parameter \textit{b}, used to wcalculate the activation of a node, see equation (\ref{eq:act}).\\[0.75ex]
\textit{Cost function} - A function to calculate the distance from the prediction and the optimimum value of an image. In this project, mean squared error is used, see equation (\ref{eq:MSE}) \\[0.75ex]
\textit{Dataset} - A cluster of data, in this case the MNIST dataset is used.\\[0.75ex]
\textit{Decay rate} - A parameter used in certain stepping schedules to determine how fast the learning rate is decreased.\\[0.75ex]
\textit{Keras} - A deep learning API for python within TensorFlow, with a library of functions for building, training and testing neural networks.\\[0.75ex]
\textit{Layers} - Layers build the network; input layer, hidden layers, output layer. \\[0.75ex]
\textit{Learning rate} - A parameter in SGD which determines how large steps are taken in the opposite direction of gradient.\\[0.75ex]
\textit{Loss function} - Same as cost function\\[0.75ex]
\textit{Mini-batch} - A group of randomly selected images used to calculate a gradient. \\[0.75ex]
\textit{MNIST} - a dataset containing 70000 labelled images of handwritten digits\\[0.75ex]
\textit{Noise ball} - A convergence size for SGD, see section \ref{sec: noiseball}\\[0.75ex]
\textit{Overfitting} - Overfitting occurs when a network becomes too optimised for a training dataset, to the detriment of performance for new data. \\[0.75ex]
\textit{ReLu} - A common activation function, see equation (\ref{eq:ReLu})\\[0.75ex]
\textit{Sigmoid function} - A common activation function, see equation (\ref{eq:sigmoid})\\[0.75ex]
\textit{Stepping schedules} - Preset schemes of how to lower the learning rate during training, see section \ref{sec: Schedules}\\[0.75ex]
\textit{Stochastic gradient descent (SGD)} - A common method in neural network training \\[0.75ex]
\textit{TensorFlow} - An end-to-end open source platform for machine learning by Google\\[0.75ex]
\textit{Weight} - A node parameter \textit{w}, used to calculate the activation of a node, see equation (\ref{eq:act}).\\\\



\end{document}
